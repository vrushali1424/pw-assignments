{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJH7MntrAmFj"
      },
      "source": [
        "###  Question 1\n",
        "**What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Ensemble Learning combines multiple weak models (base learners) to form a strong predictive model.\n",
        "\n",
        "The key idea: a group of weak models together outperform individual models by reducing bias and variance.\n",
        "\n",
        "**Common methods:** Bagging, Boosting, Stacking."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltMUeQ_0AmFj"
      },
      "source": [
        "###  Question 2\n",
        "**What is the difference between Bagging and Boosting?**\n",
        "\n",
        "| Aspect | Bagging | Boosting |\n",
        "|--------|----------|-----------|\n",
        "| Type | Parallel | Sequential |\n",
        "| Goal | Reduce variance | Reduce bias & variance |\n",
        "| Sampling | Random (with replacement) | Weighted (focus on errors) |\n",
        "| Dependence | Independent models | Each model depends on previous |\n",
        "| Example | Random Forest | AdaBoost, XGBoost |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Bhc6oWAmFk"
      },
      "source": [
        "###  Question 3\n",
        "**What is bootstrap sampling and its role in Bagging methods like Random Forest?**\n",
        "\n",
        "**Answer:**\n",
        "Bootstrap sampling = randomly drawing samples with replacement.\n",
        "\n",
        "Each tree gets a different subset → increases diversity → reduces variance → prevents overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa0uOZUdAmFl"
      },
      "source": [
        "###  Question 4\n",
        "**What are Out-of-Bag (OOB) samples and how is OOB score used?**\n",
        "\n",
        "OOB samples = data not chosen in bootstrap for a tree.\n",
        "Used to test model internally (≈ validation score).\n",
        "OOB score gives unbiased accuracy without needing extra test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tkwwe9eAmFl"
      },
      "source": [
        "###  Question 5\n",
        "**Compare feature importance in Decision Tree vs Random Forest**\n",
        "\n",
        "| Aspect | Decision Tree | Random Forest |\n",
        "|---------|----------------|----------------|\n",
        "| Basis | Single model impurity | Average across many trees |\n",
        "| Stability | Sensitive | Stable |\n",
        "| Bias | High | Low |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Question 6\n",
        "**Write a Python program to:**\n",
        "\n",
        "Load the Breast Cancer dataset\n",
        "\n",
        "Train a Random Forest Classifier\n",
        "\n",
        "Print the top 5 most important features"
      ],
      "metadata": {
        "id": "F9GAi6y9A4jF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5wWIzP6AmFm",
        "outputId": "1eba886e-ecd5-4c51-c558-457d76fbc498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "importances = pd.Series(rf.feature_importances_, index=data.feature_names)\n",
        "print(importances.sort_values(ascending=False).head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7\n",
        "**Write a Python program to:**\n",
        "\n",
        "Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "R_RYmLVOBYp0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93zXGg38AmFn",
        "outputId": "5a221c4e-b49b-4a30-b662-411a0bd8b65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "acc_dt = accuracy_score(y_test, dt.predict(X_test))\n",
        "\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "acc_bag = accuracy_score(y_test, bag.predict(X_test))\n",
        "\n",
        "print('Decision Tree Accuracy:', acc_dt)\n",
        "print('Bagging Classifier Accuracy:', acc_bag)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 8\n",
        "**Write a Python program to:**\n",
        "\n",
        "Train a Random Forest Classifier\n",
        "\n",
        "Tune max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "Print best parameters and final accuracy"
      ],
      "metadata": {
        "id": "HdL01XzZB6MW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3D16l7uVAmFp",
        "outputId": "72deaa89-38c5-4b55-dda0-b9c2ea8fc72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'max_depth': 3, 'n_estimators': 50}\n",
            "Best Accuracy: 0.9666666666666668\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [3, 5, 7, None]}\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X, y)\n",
        "print('Best Params:', grid.best_params_)\n",
        "print('Best Accuracy:', grid.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 9\n",
        "**Write a Python program to:**\n",
        "\n",
        "Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "\n",
        "Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "Y3TheJy6CGte"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc3Jn_BDAmFq",
        "outputId": "c08d9255-2d0f-4aca-9c36-9b4abec9bb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.28623579601385674\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "bag = BaggingRegressor(random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "print('Bagging Regressor MSE:', mean_squared_error(y_test, bag.predict(X_test)))\n",
        "print('Random Forest Regressor MSE:', mean_squared_error(y_test, rf.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY6_Q1xTAmFr"
      },
      "source": [
        "###  Question 10\n",
        "**Case Study: Loan Default Prediction**\n",
        "\n",
        "**Approach:**\n",
        "1. Use Boosting (e.g., XGBoost) since it handles class imbalance well.\n",
        "2. Prevent overfitting via regularization, early stopping, and CV.\n",
        "3. Use Decision Trees as base models.\n",
        "4. Evaluate with Stratified K-Fold and metrics like F1, ROC-AUC.\n",
        "5. Ensemble improves accuracy and reduces risk in loan approval decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "**Step 1**:\n",
        "\n",
        " Choosing Between Bagging or Boosting\n",
        "\n",
        "Use Boosting (e.g., XGBoost) because loan default prediction often involves imbalanced data and Boosting focuses on difficult cases.\n",
        "\n",
        "**Step 2**:\n",
        "\n",
        " Handle Overfitting\n",
        "\n",
        "Apply regularization (learning_rate, max_depth).\n",
        "\n",
        "Use early stopping.\n",
        "\n",
        "Use cross-validation to monitor performance.\n",
        "\n",
        "**Step 3**:\n",
        "\n",
        " Select Base Models\n",
        "\n",
        "Use Decision Trees as base learners for interpretability and flexibility.\n",
        "\n",
        "**Step 4**:\n",
        "\n",
        " Evaluate Performance\n",
        "\n",
        "Use Stratified K-Fold CV.\n",
        "\n",
        "Evaluate using Precision, Recall, F1-Score, and ROC-AUC.\n",
        "\n",
        "**Step 5**:\n",
        "\n",
        " Why Ensemble Learning Helps\n",
        "\n",
        "Combines multiple weak learners → stronger prediction.\n",
        "\n",
        "Handles class imbalance better.\n",
        "\n",
        "Provides stable, reliable risk predictions.\n",
        "\n",
        "Improves loan approval decision-making and reduces default losses."
      ],
      "metadata": {
        "id": "TH2ZnUezCkEj"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}