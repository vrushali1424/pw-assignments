{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## NLP Introduction & Text Processing Assignment  \n",
        "----------------------------------------\n"
      ],
      "metadata": {
        "id": "g54Yr64fAVZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: What is Computational Linguistics and how does it relate to NLP?"
      ],
      "metadata": {
        "id": "0XHTSqAlAccK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer:\n",
        "\n",
        "Computational Linguistics is an interdisciplinary field that combines linguistics and computer science to analyze and model natural language using computational methods.\n",
        "\n",
        "Natural Language Processing (NLP) is a subfield of Artificial Intelligence that applies computational linguistics techniques to build practical applications such as chatbots, translation systems, and sentiment analysis tools.\n",
        "\n",
        "Computational Linguistics provides the theoretical foundation, while NLP focuses on real-world applications."
      ],
      "metadata": {
        "id": "t--H3L6pAfHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Historical Evolution of NLP"
      ],
      "metadata": {
        "id": "yemK4-0zAiA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer:\n",
        "\n",
        "1. **Rule-Based Systems (1950s–1980s)**  \n",
        "   Used manually written grammar rules.\n",
        "\n",
        "2. **Statistical NLP (1990s–2000s)**  \n",
        "   Used probability and machine learning models like HMM and Naive Bayes.\n",
        "\n",
        "3. **Machine Learning Era (2000–2015)**  \n",
        "   Algorithms like SVM, Decision Trees were used.\n",
        "\n",
        "4. **Deep Learning Era (2015–Present)**  \n",
        "   Neural networks, Word2Vec, BERT, GPT revolutionized NLP."
      ],
      "metadata": {
        "id": "TR3K_-49AjtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: : List and explain three major use cases of NLP in today’s tech industry."
      ],
      "metadata": {
        "id": "yi_15aKgAmtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer:\n",
        "\n",
        "1. **Chatbots & Virtual Assistants** – Automated customer support.\n",
        "2. **Sentiment Analysis** – Analyze customer opinions.\n",
        "3. **Machine Translation** – Translate text between languages."
      ],
      "metadata": {
        "id": "il_bWPELAwCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: What is text normalization and why is it essential in text processing tasks?"
      ],
      "metadata": {
        "id": "nIok1lDkAycr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer:\n",
        "\n",
        "Text normalization is the process of converting text into a consistent format.\n",
        "\n",
        "Steps include:\n",
        "- Lowercasing\n",
        "- Removing punctuation\n",
        "- Removing stopwords\n",
        "- Stemming or lemmatization\n",
        "\n",
        "It improves model accuracy and reduces noise."
      ],
      "metadata": {
        "id": "TbFDuI8_A4yB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Compare and contrast stemming and lemmatization with suitable examples."
      ],
      "metadata": {
        "id": "gBlDqKr_A7ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer:\n",
        "\n",
        "| Feature | Stemming | Lemmatization |\n",
        "|----------|------------|----------------|\n",
        "| Speed | Fast | Slower |\n",
        "| Accuracy | Less accurate | More accurate |\n",
        "| Example (running) | runn | run |\n",
        "| Example (better) | better | good |"
      ],
      "metadata": {
        "id": "JqZ1XU6OBCfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text:\n",
        "\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”"
      ],
      "metadata": {
        "id": "SMd9eTmRBFNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues,\n",
        "or reach out to our HR at hr@xyz.com. You can also connect with John at\n",
        "john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "emails = re.findall(pattern, text)\n",
        "\n",
        "print(\"Extracted Emails:\")\n",
        "for email in emails:\n",
        "    print(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbprzp3vBSP7",
        "outputId": "654332a0-be69-4984-f808-738494949a2f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Emails:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7: Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”"
      ],
      "metadata": {
        "id": "mBw0pktDBVOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # <-- THIS LINE FIXES YOUR ERROR\n",
        "\n",
        "text = \"\"\"Natural Language Processing (NLP) is a fascinating field that combines\n",
        "linguistics, computer science, and artificial intelligence. It enables machines\n",
        "to understand, interpret, and generate human language. Applications of NLP\n",
        "include chatbots, sentiment analysis, and machine translation. As technology\n",
        "advances, the role of NLP in modern solutions is becoming increasingly critical.\"\"\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Frequency Distribution\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\nTop 10 Most Common Words:\")\n",
        "print(fdist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHMQYugJBvLD",
        "outputId": "51fc98eb-754f-45b6-ac40-b013b727689f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'it', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'applications', 'of', 'nlp', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'as', 'technology', 'advances', ',', 'the', 'role', 'of', 'nlp', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Top 10 Most Common Words:\n",
            "[(',', 7), ('.', 4), ('nlp', 3), ('and', 3), ('language', 2), ('is', 2), ('of', 2), ('natural', 1), ('processing', 1), ('(', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8: Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text."
      ],
      "metadata": {
        "id": "caXk51bvBzrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"John works at Microsoft in New York.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Proper Nouns Identified:\")\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":\n",
        "        print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6phkL8dyB6K0",
        "outputId": "4ed0fa26-7d15-4270-afdb-18ea92cab064"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.24.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (26.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.24.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.1.1)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Proper Nouns Identified:\n",
            "John\n",
            "Microsoft\n",
            "New\n",
            "York\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences:\n",
        "dataset = [\n",
        "\"Natural language processing enables computers to understand human language\",\n",
        "\"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        "\"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "\"Text preprocessing is a critical step before training word embeddings\",\n",
        "\"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using\n",
        "Gensim."
      ],
      "metadata": {
        "id": "ph-lYwhiCNTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "dataset = [\n",
        "\"Natural language processing enables computers to understand human language\",\n",
        "\"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "\"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "\"Text preprocessing is a critical step before training word embeddings\",\n",
        "\"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "processed_data = [word_tokenize(sentence.lower()) for sentence in dataset]\n",
        "\n",
        "model = Word2Vec(sentences=processed_data, vector_size=50, window=3, min_count=1, workers=4)\n",
        "\n",
        "print(\"Vector for word 'word':\")\n",
        "print(model.wv['word'])\n",
        "\n",
        "print(\"\\nMost similar words to 'word':\")\n",
        "print(model.wv.most_similar('word'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv0lj-CkCU_A",
        "outputId": "e0082de7-2a60-48e3-9767-d6bac9b6fc74"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Vector for word 'word':\n",
            "[-1.0403453e-03  5.0025561e-04  1.0216394e-02  1.8061617e-02\n",
            " -1.8591337e-02 -1.4261860e-02  1.2926748e-02  1.7953532e-02\n",
            " -1.0032079e-02 -7.5593363e-03  1.4738411e-02 -3.0673896e-03\n",
            " -9.0098204e-03  1.3120006e-02 -9.7130043e-03 -3.6296644e-03\n",
            "  5.7513574e-03  1.9671449e-03 -1.6544867e-02 -1.8923346e-02\n",
            "  1.4675597e-02  1.0175380e-02  1.3475348e-02  1.5095623e-03\n",
            "  1.2712638e-02 -6.8120449e-03 -1.9073798e-03  1.1505213e-02\n",
            " -1.4995098e-02 -7.8761997e-03 -1.5040041e-02 -1.8752393e-03\n",
            "  1.9053303e-02 -1.4657243e-02 -4.6755383e-03 -3.8616473e-03\n",
            "  1.6139111e-02 -1.1870659e-02  8.4576837e-05 -9.5094936e-03\n",
            " -1.9201957e-02  1.0023610e-02 -1.7489666e-02 -8.7370779e-03\n",
            " -5.9655133e-05 -6.1921793e-04 -1.5370466e-02  1.9222580e-02\n",
            "  9.9867722e-03  1.8490216e-02]\n",
            "\n",
            "Most similar words to 'word':\n",
            "[('before', 0.27079513669013977), ('enables', 0.2546687424182892), ('meaning', 0.24092619121074677), ('normalization', 0.21127448976039886), ('nlp', 0.1860644370317459), ('are', 0.17569540441036224), ('raw', 0.16715116798877716), ('applications', 0.1612214893102646), ('help', 0.1501912623643875), ('popular', 0.14508964121341705)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews."
      ],
      "metadata": {
        "id": "GvTCJpyvCcXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "reviews = [\n",
        "\"The app is very easy to use and secure.\",\n",
        "\"Customer service is terrible and slow.\",\n",
        "\"I love the new update, very smooth experience.\"\n",
        "]\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "for review in reviews:\n",
        "    score = sia.polarity_scores(review)\n",
        "    print(\"Review:\", review)\n",
        "    print(\"Sentiment Score:\", score)\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVIvEQq4Ciy2",
        "outputId": "07620e77-260d-4095-8b09-2248936154f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: The app is very easy to use and secure.\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.556, 'pos': 0.444, 'compound': 0.6801}\n",
            "------\n",
            "Review: Customer service is terrible and slow.\n",
            "Sentiment Score: {'neg': 0.383, 'neu': 0.617, 'pos': 0.0, 'compound': -0.4767}\n",
            "------\n",
            "Review: I love the new update, very smooth experience.\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.588, 'pos': 0.412, 'compound': 0.6369}\n",
            "------\n"
          ]
        }
      ]
    }
  ]
}